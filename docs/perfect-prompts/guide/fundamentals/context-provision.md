# Provis√£o e Gest√£o de Contexto

**Otimiza√ß√£o da Janela de Contexto** | **Vers√£o 1.0** | **Dutt eCommerce Website Design**

---

## üéØ Import√¢ncia da Context Provision

**Context Provision** √© a arte e ci√™ncia de fornecer informa√ß√µes relevantes, organizadas e otimizadas dentro da janela de contexto limitada dos LLMs. √â crucial para an√°lises precisas, decis√µes bem fundamentadas e outputs de alta qualidade em aplica√ß√µes corporativas.

### **Por que Context Management Importa**
- **Precis√£o**: Informa√ß√µes relevantes produzem an√°lises mais acuradas
- **Efici√™ncia**: Contexto otimizado reduz custo computacional
- **Relev√¢ncia**: Foco em dados pertinentes ao problema espec√≠fico
- **Confiabilidade**: Base s√≥lida de informa√ß√µes reduz alucina√ß√µes  
- **Rastreabilidade**: Contexto documentado facilita auditoria
- **Escalabilidade**: Gest√£o eficiente permite processar volumes maiores

---

## üìê Limita√ß√µes de Contexto por Modelo

### **Context Windows Atuais (2025)**

| Modelo | Context Window | Uso Recomendado | Custo Relativo |
|---------|---------------|-----------------|----------------|
| **Claude 3.5 Sonnet** | 200K tokens | An√°lise de documentos longos | Alto |
| **GPT-4 Turbo** | 128K tokens | Aplica√ß√µes gerais balanceadas | M√©dio-Alto |
| **GPT-4** | 8K tokens | Tarefas espec√≠ficas e r√°pidas | M√©dio |
| **Gemini Pro** | 2M tokens | Processing massivo de dados | Vari√°vel |
| **Llama 2** | 4K tokens | Deploy local, casos espec√≠ficos | Baixo |

### **Estimativa de Tokens (Portugu√™s Brasileiro)**

```python
def estimate_tokens(text):
    """Estimativa aproximada para portugu√™s brasileiro"""
    # Regra pr√°tica: ~4 caracteres = 1 token em portugu√™s
    char_count = len(text)
    estimated_tokens = char_count / 4
    
    # Ajuste para complexidade do texto
    if any(char in text for char in ['#', '{', '}', '<', '>']):
        estimated_tokens *= 1.1  # Estruturas aumentam token count
    
    if text.count('\n') / len(text) > 0.05:
        estimated_tokens *= 1.05  # Muitas quebras de linha
    
    return int(estimated_tokens)

# Benchmarks pr√°ticos
examples = {
    "Email corporativo": "150-300 tokens",
    "Relat√≥rio executivo (2 p√°ginas)": "800-1200 tokens", 
    "Contrato padr√£o": "2000-4000 tokens",
    "Balan√ßo patrimonial completo": "1500-2500 tokens",
    "Manual de procedimentos": "5000-15000 tokens"
}
```

---

## üèóÔ∏è Estrat√©gias de Context Provision

### **1. Hierarchical Context (Contexto Hier√°rquico)**

#### **Pir√¢mide de Relev√¢ncia**
```
üî∫ CONTEXTO CR√çTICO (Top 20%)
‚îú‚îÄ‚îÄ Informa√ß√£o essencial para decis√£o
‚îú‚îÄ‚îÄ Dados que impactam diretamente o resultado  
‚îî‚îÄ‚îÄ Restri√ß√µes e limita√ß√µes obrigat√≥rias

üî∏ CONTEXTO IMPORTANTE (Middle 60%)
‚îú‚îÄ‚îÄ Background necess√°rio para compreens√£o
‚îú‚îÄ‚îÄ Dados hist√≥ricos relevantes
‚îî‚îÄ‚îÄ Especifica√ß√µes t√©cnicas detalhadas

üîπ CONTEXTO COMPLEMENTAR (Bottom 20%)
‚îú‚îÄ‚îÄ Informa√ß√µes "nice to have"
‚îú‚îÄ‚îÄ Detalhes adicionais
‚îî‚îÄ‚îÄ Referencias e fontes
```

#### **Implementa√ß√£o Pr√°tica - An√°lise Financeira**
```markdown
## CONTEXTO CR√çTICO - OBRIGAT√ìRIO (500 tokens)
### Opera√ß√£o Analisada
- Valor: R$ 2.5M  
- Origem: Conta PJ (CNPJ: 12.345.678/0001-90)
- Destino: Banco Internacional (SWIFT: CAYBKY22)
- Data: 2024-09-24
- Justificativa: "Pagamento de fornecedor internacional"

### Normas BCB Aplic√°veis - CR√çTICAS
- Circular 3.542/2014: Preven√ß√£o lavagem de dinheiro
- Lei 9.613/1998: Tipifica√ß√£o de crimes financeiros
- Resolu√ß√£o 4.595/2017: Opera√ß√µes com exterior

## CONTEXTO IMPORTANTE - RELEVANTE (1000 tokens)  
### Hist√≥rico do Cliente
- Empresa: Importadora de eletr√¥nicos (desde 2018)
- Faturamento anual: R$ 15M-25M
- Perfil operacional: 8-12 opera√ß√µes/m√™s, t√≠quete m√©dio R$ 400K
- Relacionamento banc√°rio: 6 anos, sem ocorr√™ncias

### An√°lise de Risco Pr√©via
- Score interno: 6.5/10 (m√©dio-alto)
- √öltima opera√ß√£o similar: R$ 1.8M (3 meses atr√°s)
- Setor econ√¥mico: M√©dio risco conforme CNAE
- Regi√£o: S√£o Paulo (baixo risco geogr√°fico)

### Contexto Regulat√≥rio  
- Banco Central alertas recentes: Para√≠sos fiscais
- COAF warnings: Opera√ß√µes >R$ 2M requerem aten√ß√£o especial
- Tend√™ncias de mercado: Aumento 15% em opera√ß√µes suspeitas Q3/2024

## CONTEXTO COMPLEMENTAR - OPCIONAL (500 tokens)
### Documenta√ß√£o Dispon√≠vel
- Contrato de importa√ß√£o (verificado)
- Invoice do fornecedor (validada)
- Licen√ßa de importa√ß√£o (conforme)
- Documenta√ß√£o KYC atualizada

### Refer√™ncias Setoriais
- Benchmark setor eletr√¥nicos: 2-3% opera√ß√µes requerem an√°lise adicional
- Padr√£o similar institui√ß√µes: Bloqueio preventivo comum para >R$ 2M
- Guidelines internacionais: FATF recommendations aplic√°veis
```

### **2. Contextual Chunking (Fragmenta√ß√£o Contextual)**

#### **T√©cnica: Progressive Context Loading**
```python
class ContextualChunker:
    def __init__(self, max_tokens=8000):
        self.max_tokens = max_tokens
        self.context_priority = {
            "critical": 1,
            "important": 2, 
            "supplementary": 3
        }
    
    def chunk_by_priority(self, context_dict):
        """Fragmenta contexto por prioridade respeitando token limits"""
        chunks = []
        current_chunk = {"tokens": 0, "content": []}
        
        # Ordenar por prioridade
        sorted_context = sorted(
            context_dict.items(),
            key=lambda x: self.context_priority.get(x[0], 99)
        )
        
        for context_type, content in sorted_context:
            content_tokens = estimate_tokens(content)
            
            if current_chunk["tokens"] + content_tokens > self.max_tokens:
                # Finalizar chunk atual  
                if current_chunk["content"]:
                    chunks.append(current_chunk)
                
                # Iniciar novo chunk
                current_chunk = {
                    "tokens": content_tokens,
                    "content": [(context_type, content)]
                }
            else:
                current_chunk["tokens"] += content_tokens
                current_chunk["content"].append((context_type, content))
        
        # Adicionar √∫ltimo chunk
        if current_chunk["content"]:
            chunks.append(current_chunk)
        
        return chunks

# Uso pr√°tico
context_data = {
    "critical": "Opera√ß√£o R$ 2.5M para para√≠so fiscal...",
    "important": "Hist√≥rico do cliente nos √∫ltimos 2 anos...",
    "supplementary": "Documenta√ß√£o adicional e referencias..."
}

chunker = ContextualChunker(max_tokens=6000)
chunks = chunker.chunk_by_priority(context_data)

for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {chunk['tokens']} tokens")
```

### **3. Dynamic Context Selection (Sele√ß√£o Din√¢mica)**

#### **Context Relevance Scoring**
```python
def score_context_relevance(query, context_pieces):
    """Score relev√¢ncia de cada pe√ßa de contexto para query espec√≠fica"""
    scores = {}
    
    # Keywords extraction from query
    query_keywords = extract_keywords(query.lower())
    
    for context_id, context_text in context_pieces.items():
        score = 0
        context_lower = context_text.lower()
        
        # Keyword matching (weighted)
        for keyword in query_keywords:
            if keyword in context_lower:
                # Pontua√ß√£o por frequ√™ncia e posi√ß√£o
                frequency = context_lower.count(keyword)
                position_bonus = 2 if context_lower.find(keyword) < 200 else 1
                score += frequency * position_bonus
        
        # Domain-specific scoring
        if "financeiro" in query.lower():
            financial_terms = ["bcb", "risco", "compliance", "opera√ß√£o", "norma"]
            for term in financial_terms:
                if term in context_lower:
                    score += 3
        
        # Recency bonus (if applicable)  
        if has_recent_date(context_text):
            score += 2
            
        scores[context_id] = score
    
    # Return top-scoring contexts within token budget
    return select_top_contexts(scores, token_budget=6000)

# Implementa√ß√£o para compliance banc√°rio
query = "Analise risco desta opera√ß√£o internacional de R$ 2.5M"
available_contexts = {
    "operacao_details": "Detalhes da opera√ß√£o...",
    "cliente_history": "Hist√≥rico dos √∫ltimos 5 anos...", 
    "normas_bcb": "Regulamenta√ß√µes aplic√°veis...",
    "market_trends": "Tend√™ncias de mercado...",
    "documentation": "Documentos suportivos..."
}

relevant_contexts = score_context_relevance(query, available_contexts)
```

---

## üìã Context Templates por Setor

### **üè¶ Setor Financeiro - Template de Contexto**

```markdown
# CONTEXT TEMPLATE: An√°lise de Compliance Banc√°rio

## SE√á√ÉO 1: CONTEXTO OPERACIONAL CR√çTICO [300-500 tokens]
### Opera√ß√£o em An√°lise  
- **ID**: {{OPERATION_ID}}
- **Valor**: R$ {{AMOUNT:,.2f}}  
- **Data**: {{OPERATION_DATE}}
- **Origem**: {{ORIGIN_ACCOUNT}} ({{ORIGIN_ENTITY}})
- **Destino**: {{DESTINATION_BANK}} ({{DESTINATION_COUNTRY}})
- **Tipo**: {{OPERATION_TYPE}}
- **Justificativa**: "{{STATED_PURPOSE}}"

### Classifica√ß√£o Imediata
- **Valor vs Perfil**: {{AMOUNT_VS_PROFILE}} (Normal/Elevado/Cr√≠tico)
- **Destino**: {{DESTINATION_RISK}} (Baixo/M√©dio/Alto/Para√≠so Fiscal)
- **Urg√™ncia**: {{URGENCY_LEVEL}} (Normal/Alta/Cr√≠tica)

## SE√á√ÉO 2: CONTEXTO REGULAT√ìRIO [400-600 tokens]
### Normas BCB Diretamente Aplic√°veis
```yaml
primary_regulations:
  - norm: "{{PRIMARY_BCB_NORM}}"
    article: "{{SPECIFIC_ARTICLE}}"
    summary: "{{NORM_SUMMARY}}"
    threshold: "{{APPLICABLE_THRESHOLD}}"
  
secondary_regulations:
  - norm: "{{SECONDARY_BCB_NORM}}"
    relevance: "{{WHY_RELEVANT}}"
```

### Contexto de Enforcement
- **Alertas BCB Recentes**: {{RECENT_BCB_ALERTS}}
- **Tend√™ncias Regulat√≥rias**: {{REGULATORY_TRENDS}}
- **Precedentes Similares**: {{SIMILAR_CASES_OUTCOME}}

## SE√á√ÉO 3: CONTEXTO DO CLIENTE [500-800 tokens]
### Perfil Corporativo
```json
{
  "razao_social": "{{COMPANY_NAME}}",
  "cnpj": "{{CNPJ}}",  
  "cnae": "{{CNAE_CODE}} - {{BUSINESS_DESCRIPTION}}",
  "porte": "{{COMPANY_SIZE}}",
  "faturamento_anual": "{{ANNUAL_REVENUE}}",
  "tempo_relacionamento": "{{RELATIONSHIP_YEARS}} anos"
}
```

### Hist√≥rico Operacional (12 meses)
- **Total Opera√ß√µes**: {{TOTAL_OPERATIONS_12M}}
- **Valor M√©dio**: R$ {{AVERAGE_AMOUNT:,.2f}}  
- **Frequ√™ncia**: {{FREQUENCY_PATTERN}}
- **Sazonalidade**: {{SEASONAL_PATTERNS}}
- **Ocorr√™ncias**: {{PAST_INCIDENTS}} (Nenhuma/Leves/Graves)

### Score de Risco Interno
- **Score Atual**: {{INTERNAL_RISK_SCORE}}/10
- **√öltima Atualiza√ß√£o**: {{SCORE_LAST_UPDATE}}
- **Fatores de Risco**: {{RISK_FACTORS_LIST}}
- **Fatores Mitigantes**: {{MITIGATING_FACTORS}}

## SE√á√ÉO 4: CONTEXTO DE MERCADO [200-400 tokens]
### Intelligence Recente
- **Alertas COAF**: {{COAF_RECENT_ALERTS}}
- **Padr√µes Setoriais**: {{INDUSTRY_RISK_PATTERNS}}  
- **Geopol√≠tica**: {{RELEVANT_GEOPOLITICAL_CONTEXT}}
- **Benchmarks**: {{PEER_COMPARISON_DATA}}

## SE√á√ÉO 5: DOCUMENTA√á√ÉO SUPORTIVA [100-300 tokens]
### Evid√™ncias Dispon√≠veis
- ‚úÖ Contrato comercial: {{CONTRACT_STATUS}}
- ‚úÖ Invoice/Fatura: {{INVOICE_STATUS}} 
- ‚úÖ KYC atualizado: {{KYC_STATUS}}
- ‚úÖ Licen√ßas: {{LICENSES_STATUS}}
- ‚ùì Documentos pendentes: {{PENDING_DOCS}}

---
**Total Estimated Tokens**: {{TOTAL_CONTEXT_TOKENS}}
**Priority Level**: {{CONTEXT_PRIORITY}}  
**Last Updated**: {{CONTEXT_TIMESTAMP}}
```

### **üõí eCommerce - Template de Contexto**

```markdown
# CONTEXT TEMPLATE: Otimiza√ß√£o de Produto eCommerce

## SE√á√ÉO 1: CONTEXTO DO PRODUTO [400-600 tokens]
### Produto em An√°lise
```yaml
produto:
  nome_atual: "{{CURRENT_PRODUCT_NAME}}"
  categoria: "{{PRODUCT_CATEGORY}}"
  subcategoria: "{{SUBCATEGORY}}" 
  marca: "{{BRAND_NAME}}"
  preco_atual: "R$ {{CURRENT_PRICE:,.2f}}"
  margem: "{{PROFIT_MARGIN}}%"
  estoque: "{{STOCK_LEVEL}}"
  
performance_atual:
  visualizacoes_mes: {{MONTHLY_VIEWS}}
  conversao_atual: "{{CURRENT_CONVERSION}}%"
  bounce_rate: "{{BOUNCE_RATE}}%"
  tempo_pagina: "{{AVG_TIME_ON_PAGE}}s"
  reviews_count: {{REVIEWS_COUNT}}
  rating_medio: {{AVERAGE_RATING}}/5
```

### Conte√∫do Atual (para otimiza√ß√£o)
- **T√≠tulo**: "{{CURRENT_TITLE}}"
- **Descri√ß√£o**: "{{CURRENT_DESCRIPTION}}"
- **Bullet Points**: {{CURRENT_BULLETS}}
- **Imagens**: {{IMAGE_COUNT}} ({{IMAGE_QUALITY_SCORE}}/10)

## SE√á√ÉO 2: CONTEXTO COMPETITIVO [500-700 tokens]
### An√°lise da Concorr√™ncia
```json
{
  "concorrentes_diretos": [
    {
      "nome": "{{COMPETITOR_1_NAME}}",
      "preco": "R$ {{COMPETITOR_1_PRICE}}",
      "posicao_organica": {{ORGANIC_POSITION}},
      "diferenciais": ["{{DIFF_1}}", "{{DIFF_2}}"],
      "pontos_fracos": ["{{WEAK_1}}", "{{WEAK_2}}"]
    },
    {
      "nome": "{{COMPETITOR_2_NAME}}",  
      "preco": "R$ {{COMPETITOR_2_PRICE}}",
      "market_share": "{{MARKET_SHARE}}%",
      "estrategia_pricing": "{{PRICING_STRATEGY}}"
    }
  ],
  "gap_analysis": {
    "opportunities": ["{{OPPORTUNITY_1}}", "{{OPPORTUNITY_2}}"],
    "threats": ["{{THREAT_1}}", "{{THREAT_2}}"]
  }
}
```

### Posicionamento SEO
- **Palavra-chave Principal**: "{{PRIMARY_KEYWORD}}" ({{SEARCH_VOLUME}}/m√™s)
- **Posi√ß√£o Atual**: {{CURRENT_RANKING}}¬∫
- **Keywords Secund√°rias**: {{SECONDARY_KEYWORDS}}
- **Featured Snippets**: {{SNIPPET_OPPORTUNITIES}}

## SE√á√ÉO 3: CONTEXTO DO P√öBLICO [300-500 tokens]  
### Target Audience
```yaml
publico_primario:
  idade: "{{TARGET_AGE_RANGE}}"
  genero: "{{GENDER_DISTRIBUTION}}"
  renda_familiar: "R$ {{INCOME_RANGE}}"
  escolaridade: "{{EDUCATION_LEVEL}}"
  comportamento_compra: "{{BUYING_BEHAVIOR}}"
  
pain_points:
  - "{{PAIN_POINT_1}}"
  - "{{PAIN_POINT_2}}"  
  - "{{PAIN_POINT_3}}"
  
motivacoes_compra:
  - "{{MOTIVATION_1}}"
  - "{{MOTIVATION_2}}"
  
canais_preferidos:
  - "{{PREFERRED_CHANNEL_1}}"
  - "{{PREFERRED_CHANNEL_2}}"
```

### Comportamento no Site
- **Device**: {{MOBILE_TRAFFIC}}% mobile, {{DESKTOP_TRAFFIC}}% desktop
- **Geografia**: {{TOP_CITIES}} (principais cidades)  
- **Hor√°rios**: {{PEAK_HOURS}} (picos de tr√°fego)
- **Jornada T√≠pica**: {{TYPICAL_USER_JOURNEY}}

## SE√á√ÉO 4: CONTEXTO DE NEG√ìCIO [200-400 tokens]
### Objetivos da Otimiza√ß√£o
- **Meta Convers√£o**: {{TARGET_CONVERSION}}% (atual: {{CURRENT_CONVERSION}}%)
- **Meta Revenue**: R$ {{REVENUE_TARGET:,.2f}}/m√™s
- **ROI Esperado**: {{EXPECTED_ROI}}% em {{TIMEFRAME}}
- **KPIs Secund√°rios**: {{SECONDARY_KPIS}}

### Restri√ß√µes e Limita√ß√µes
- **Budget**: R$ {{OPTIMIZATION_BUDGET:,.2f}}
- **Timeline**: {{PROJECT_TIMELINE}}  
- **Recursos**: {{AVAILABLE_RESOURCES}}
- **Guidelines de Marca**: {{BRAND_RESTRICTIONS}}
- **Conformidade**: {{COMPLIANCE_REQUIREMENTS}}

---
**Context Quality Score**: {{CONTEXT_COMPLETENESS}}/10
**Optimization Readiness**: {{READINESS_LEVEL}}
**Estimated Token Usage**: {{CONTEXT_TOKENS}}
```

---

## üîß Context Optimization Techniques

### **1. Context Compression**

```python
def compress_context(full_context, target_tokens, priority_weights=None):
    """Comprime contexto mantendo informa√ß√µes mais importantes"""
    
    if priority_weights is None:
        priority_weights = {
            "critical": 1.0,
            "important": 0.7,
            "supplementary": 0.4
        }
    
    compressed_sections = {}
    token_budget = target_tokens
    
    # Primeiro passo: garantir se√ß√µes cr√≠ticas
    for section, content in full_context.items():
        if section in priority_weights:
            weight = priority_weights[section]
            section_tokens = estimate_tokens(content)
            
            if weight >= 0.8:  # Cr√≠tico - incluir sempre
                compressed_sections[section] = content
                token_budget -= section_tokens
            elif weight >= 0.5:  # Importante - comprimir se necess√°rio
                if section_tokens <= token_budget * 0.3:
                    compressed_sections[section] = content
                    token_budget -= section_tokens
                else:
                    # Comprimir mantendo pontos-chave
                    compressed = extract_key_points(content, int(section_tokens * 0.6))
                    compressed_sections[f"{section}_compressed"] = compressed
                    token_budget -= estimate_tokens(compressed)
    
    return compressed_sections

def extract_key_points(text, target_tokens):
    """Extrai pontos-chave de um texto longo"""
    sentences = text.split('.')
    
    # Score sentences por relev√¢ncia (simplified)
    scored_sentences = []
    for sentence in sentences:
        score = 0
        # Bonus para senten√ßas com n√∫meros/dados
        if any(char.isdigit() for char in sentence):
            score += 2
        # Bonus para palavras-chave importantes
        important_words = ['risco', 'bcb', 'norma', 'opera√ß√£o', 'valor', 'cliente']
        for word in important_words:
            if word.lower() in sentence.lower():
                score += 1
        
        scored_sentences.append((score, sentence.strip()))
    
    # Selecionar top sentences que cabem no or√ßamento
    sorted_sentences = sorted(scored_sentences, key=lambda x: x[0], reverse=True)
    selected = []
    current_tokens = 0
    
    for score, sentence in sorted_sentences:
        sentence_tokens = estimate_tokens(sentence)
        if current_tokens + sentence_tokens <= target_tokens:
            selected.append(sentence)
            current_tokens += sentence_tokens
        else:
            break
    
    return '. '.join(selected)
```

### **2. Context Retrieval (RAG Integration)**

```python
class ContextRetriever:
    def __init__(self, knowledge_base_path, embedding_model="text-embedding-ada-002"):
        self.knowledge_base = self.load_knowledge_base(knowledge_base_path)
        self.embedding_model = embedding_model
        self.vector_index = self.build_vector_index()
    
    def retrieve_relevant_context(self, query, max_contexts=5, max_tokens=2000):
        """Recupera contexto mais relevante para a query"""
        
        # Generate query embedding
        query_embedding = self.get_embedding(query)
        
        # Find most similar contexts
        similarities = self.calculate_similarities(query_embedding)
        
        # Select top contexts within token budget
        selected_contexts = []
        token_count = 0
        
        for similarity, context_id in similarities[:max_contexts]:
            context = self.knowledge_base[context_id]
            context_tokens = estimate_tokens(context["content"])
            
            if token_count + context_tokens <= max_tokens:
                selected_contexts.append({
                    "id": context_id,
                    "content": context["content"],
                    "relevance_score": similarity,
                    "source": context["source"],
                    "last_updated": context["timestamp"]
                })
                token_count += context_tokens
            else:
                break
        
        return {
            "contexts": selected_contexts,
            "total_tokens": token_count,
            "coverage_score": self.calculate_coverage_score(query, selected_contexts)
        }
    
    def adaptive_context_selection(self, query, context_complexity):
        """Seleciona contexto adaptado √† complexidade da query"""
        
        if context_complexity == "simple":
            # Para queries simples, contexto m√≠nimo
            return self.retrieve_relevant_context(query, max_contexts=2, max_tokens=800)
        elif context_complexity == "complex":
            # Para an√°lises complexas, contexto extenso
            return self.retrieve_relevant_context(query, max_contexts=8, max_tokens=4000)
        else:
            # Contexto balanceado
            return self.retrieve_relevant_context(query, max_contexts=5, max_tokens=2000)
```

### **3. Context Validation**

```python
def validate_context_quality(context_dict, query):
    """Valida qualidade e relev√¢ncia do contexto fornecido"""
    
    validation_results = {
        "completeness_score": 0,
        "relevance_score": 0,
        "currency_score": 0,
        "consistency_score": 0,
        "issues": [],
        "recommendations": []
    }
    
    # 1. Completeness Check
    required_sections = identify_required_sections(query)
    provided_sections = set(context_dict.keys())
    missing_sections = required_sections - provided_sections
    
    validation_results["completeness_score"] = 1 - (len(missing_sections) / len(required_sections))
    
    if missing_sections:
        validation_results["issues"].append(f"Missing sections: {missing_sections}")
        validation_results["recommendations"].append("Add missing critical context sections")
    
    # 2. Relevance Check  
    query_keywords = extract_keywords(query)
    total_relevance = 0
    
    for section, content in context_dict.items():
        section_relevance = calculate_keyword_overlap(query_keywords, content)
        total_relevance += section_relevance
    
    validation_results["relevance_score"] = min(total_relevance / len(context_dict), 1.0)
    
    # 3. Currency Check
    dates_found = extract_dates_from_context(context_dict)
    if dates_found:
        most_recent = max(dates_found)
        days_old = (datetime.now() - most_recent).days
        validation_results["currency_score"] = max(0, 1 - (days_old / 365))  # Decay over 1 year
    else:
        validation_results["issues"].append("No timestamp information found")
        validation_results["currency_score"] = 0.5  # Default if no dates
    
    # 4. Consistency Check
    consistency_issues = detect_consistency_issues(context_dict)
    validation_results["consistency_score"] = 1 - (len(consistency_issues) / 10)  # Normalize
    validation_results["issues"].extend(consistency_issues)
    
    # Overall Quality Score
    validation_results["overall_quality"] = (
        validation_results["completeness_score"] * 0.3 +
        validation_results["relevance_score"] * 0.4 +
        validation_results["currency_score"] * 0.2 +
        validation_results["consistency_score"] * 0.1
    )
    
    return validation_results
```

---

## üìä Context ROI e Performance

### **Context Efficiency Metrics**

```python
def measure_context_efficiency(baseline_performance, optimized_performance):
    """Mede impacto da otimiza√ß√£o de contexto"""
    
    metrics = {}
    
    # Accuracy improvement
    accuracy_gain = (optimized_performance["accuracy"] - baseline_performance["accuracy"]) / baseline_performance["accuracy"]
    metrics["accuracy_improvement"] = f"{accuracy_gain:.1%}"
    
    # Token efficiency (better results with same/fewer tokens)
    token_efficiency = optimized_performance["quality_score"] / optimized_performance["tokens_used"]
    baseline_efficiency = baseline_performance["quality_score"] / baseline_performance["tokens_used"]
    efficiency_gain = (token_efficiency - baseline_efficiency) / baseline_efficiency
    metrics["token_efficiency_gain"] = f"{efficiency_gain:.1%}"
    
    # Cost optimization
    cost_per_quality_point = calculate_cost_per_quality(optimized_performance)
    baseline_cost_per_quality = calculate_cost_per_quality(baseline_performance)
    cost_improvement = (baseline_cost_per_quality - cost_per_quality_point) / baseline_cost_per_quality
    metrics["cost_efficiency_improvement"] = f"{cost_improvement:.1%}"
    
    # Response time impact
    if "response_time" in optimized_performance:
        time_change = (optimized_performance["response_time"] - baseline_performance["response_time"]) / baseline_performance["response_time"]
        metrics["response_time_change"] = f"{time_change:.1%}"
    
    return metrics

# Example calculation
baseline = {
    "accuracy": 0.78,
    "quality_score": 7.2,
    "tokens_used": 8500,
    "response_time": 12.3
}

optimized = {
    "accuracy": 0.89,
    "quality_score": 8.7,
    "tokens_used": 6200,
    "response_time": 9.1
}

efficiency_gains = measure_context_efficiency(baseline, optimized)
```

### **Context Strategy ROI Calculator**

```python
def calculate_context_strategy_roi():
    """ROI de implementar estrat√©gias de context optimization"""
    
    # Baseline: Context management ad-hoc
    baseline_metrics = {
        "avg_tokens_per_query": 9500,
        "success_rate": 0.73,
        "revision_cycles": 2.4,
        "analyst_time_hours": 0.75,
        "context_prep_time": 0.45
    }
    
    # With optimized context strategies
    optimized_metrics = {
        "avg_tokens_per_query": 6800,
        "success_rate": 0.91,
        "revision_cycles": 1.1,
        "analyst_time_hours": 0.35,
        "context_prep_time": 0.15
    }
    
    # Calculate monthly savings
    monthly_queries = 1500
    hourly_rate = 75  # USD
    token_cost_per_1k = 0.002  # USD
    
    # Token cost savings
    token_savings = (baseline_metrics["avg_tokens_per_query"] - optimized_metrics["avg_tokens_per_query"]) * monthly_queries
    monthly_token_savings = (token_savings / 1000) * token_cost_per_1k
    
    # Time savings
    time_savings_per_query = (baseline_metrics["analyst_time_hours"] + baseline_metrics["context_prep_time"]) - (optimized_metrics["analyst_time_hours"] + optimized_metrics["context_prep_time"])
    monthly_time_savings = time_savings_per_query * monthly_queries * hourly_rate
    
    # Quality improvement value (less rework)  
    rework_reduction = (baseline_metrics["revision_cycles"] - optimized_metrics["revision_cycles"]) * 0.25  # 15min per cycle
    monthly_rework_savings = rework_reduction * monthly_queries * hourly_rate
    
    total_monthly_savings = monthly_token_savings + monthly_time_savings + monthly_rework_savings
    
    return {
        "monthly_savings": {
            "token_costs": f"${monthly_token_savings:.0f}",
            "time_savings": f"${monthly_time_savings:.0f}", 
            "rework_reduction": f"${monthly_rework_savings:.0f}",
            "total": f"${total_monthly_savings:.0f}"
        },
        "annual_roi": f"${total_monthly_savings * 12:.0f}",
        "payback_period": "1-2 months (setup and training time)",
        "efficiency_gains": {
            "token_reduction": f"{((baseline_metrics['avg_tokens_per_query'] - optimized_metrics['avg_tokens_per_query']) / baseline_metrics['avg_tokens_per_query']) * 100:.1f}%",
            "success_rate_improvement": f"{((optimized_metrics['success_rate'] - baseline_metrics['success_rate']) / baseline_metrics['success_rate']) * 100:.1f}%",
            "time_reduction": f"{(time_savings_per_query / (baseline_metrics['analyst_time_hours'] + baseline_metrics['context_prep_time'])) * 100:.1f}%"
        }
    }
```

---

## üöÄ Pr√≥ximos Passos

### **Para Implementa√ß√£o Imediata**  
1. **[Advanced XML Tags](../advanced/xml-tags-complete-guide.md)** - Estrutura√ß√£o avan√ßada de contexto
2. **[Long Context Management](../optimization/long-context-tips.md)** - Gest√£o de contextos extensos
3. **[Template Integration](prompt-templates-and-variables.md)** - Templates com context slots

### **Para Otimiza√ß√£o Avan√ßada**
1. **[Context Window Management](../optimization/context-window-management.md)** - Estrat√©gias de janela
2. **[Prompt Caching](../optimization/prompt-caching.md)** - Cache de contextos frequentes  
3. **[Batch Processing](../optimization/batch-processing.md)** - Processamento contextual em lote

### **Para Integra√ß√£o Corporativa**
1. **[RAG Integration](../tools/rag-integration.md)** - Recupera√ß√£o autom√°tica de contexto
2. **[Knowledge Management](../tools/knowledge-base.md)** - Base de conhecimento estruturada
3. **[API Context Management](../optimization/api-integration.md)** - Contexto program√°tico

---

*Context provision eficaz √© a ponte entre dados corporativos e insights acion√°veis, maximizando valor dentro das limita√ß√µes t√©cnicas dos LLMs.*

---

**Desenvolvido por Dutt eCommerce Website Design - Otimiza√ß√£o de IA para an√°lises corporativas baseadas em dados.**